x-airflow-common: &airflow-common
  build:
    context: ..
    dockerfile: docker/Dockerfile.airflow
  platform: linux/amd64
  env_file:
    - ../.env
  environment: &airflow-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    DATA_ROOT: /opt/airflow/data
  volumes:
    - ../dags:/opt/airflow/dags
    - ../jobs:/opt/airflow/jobs
    - ../utils:/opt/airflow/utils
    - ../configs:/opt/airflow/configs
    - ../schemas:/opt/airflow/schemas
    - ../data:/opt/airflow/data
  networks:
    - youtube-ai-net
  extra_hosts:
    - "host.docker.internal:host-gateway"
  depends_on:
    postgres:
      condition: service_healthy

services:
  # ── Airflow metadata database ──────────────────────────────
  postgres:
    image: postgres:17-alpine
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "0.0.0.0:5432:5432"
    networks:
      - youtube-ai-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      retries: 5
      start_period: 5s

  # ── One-shot DB initialisation ─────────────────────────────
  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate &&
        airflow users create \
          --username admin \
          --password admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com || true
    restart: "no"

  # ── Airflow API server (UI + REST API on port 8080) ────────
  airflow-api-server:
    <<: *airflow-common
    command: airflow api-server
    ports:
      - "0.0.0.0:8080:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always

  # ── Airflow scheduler ─────────────────────────────────────
  airflow-scheduler:
    <<: *airflow-common
    command: airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: always

  # ── Airflow DAG processor ─────────────────────────────────
  airflow-dag-processor:
    <<: *airflow-common
    command: airflow dag-processor
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: always

  # ── pgAdmin (DB browser on port 5050) ─────────────────────
  pgadmin:
    image: dpage/pgadmin4:latest
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_LISTEN_PORT: 5050
    ports:
      - "0.0.0.0:5050:5050"
    volumes:
      - pgadmin-data:/var/lib/pgadmin
    networks:
      - youtube-ai-net
    depends_on:
      postgres:
        condition: service_healthy
    restart: always

  # ── AWS Glue / Spark (local dev via aws-glue-libs:5) ──────
  glue:
    build:
      context: ..
      dockerfile: docker/Dockerfile.spark
    platform: linux/amd64
    entrypoint: [""]
    command: ["sleep", "infinity"]
    env_file:
      - ../.env
    environment:
      DATA_ROOT: /home/hadoop/workspace/data
    ports:
      - "0.0.0.0:4040:4040"
      - "0.0.0.0:18080:18080"
    volumes:
      - ../jobs:/home/hadoop/workspace/jobs
      - ../utils:/home/hadoop/workspace/utils
      - ../configs:/home/hadoop/workspace/configs
      - ../schemas:/home/hadoop/workspace/schemas
      - ../data:/home/hadoop/workspace/data
      - ~/.aws:/home/hadoop/.aws:ro
    networks:
      - youtube-ai-net
    extra_hosts:
      - "host.docker.internal:host-gateway"

networks:
  youtube-ai-net:
    driver: bridge

volumes:
  postgres-data:
  pgadmin-data:
